{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be7d75a",
   "metadata": {},
   "source": [
    "<h1 align='Center'>\n",
    "<img src=\"https://assets.digihaul.com/images/logo.png\" width=\"350\" height=\"550\" align=\"center\"/>\n",
    "</h1>\n",
    "<h3 align='center'> Task 3</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c41a3",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Road haulage is essential for the people and businesses of the UK. Approximately 90% of all goods transported by land in Great Britain are moved directly by road. DigiHaul is a digital transport business, specialising in managing, consolidating and integrating data from both Carriers and Shippers to deliver seamless end-to-end logistics service.\n",
    "Shippers book shipments on the DigiHaul platform, detailing the scheduled collection and delivery time windows / locations and required vehicle types for carriers to consider. Once a carrier accepts a job and collection is scheduled, DigiHaulâ€™s driver app facilitates real-time tracking of shipments through GPS signals, subject to carriers granting permissions for location logging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23e46e",
   "metadata": {},
   "source": [
    "#### Python packages:\n",
    "\n",
    "- data handling packages   : pandas, numpy, re\n",
    "- modelling packages       : sklearn, pickle\n",
    "- geo package              : google maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b54345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data handling python packages\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import googlemaps\n",
    "\n",
    "g_client = googlemaps.Client(key=\"AIzaSyCQqsfGtdwOFDsuQwcz2sHS_q6BXibukZk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba8345",
   "metadata": {},
   "source": [
    "### Input data:\n",
    "\n",
    "Three datasets are provided for working with the tasks and each one of them are in .csv format\n",
    "\n",
    "1. gps_data.csv: contains GPS logging(latitude and longitude) for each shipment along with the timestamp details\n",
    "\n",
    "2. shipment.csv: contains all the details about each shipment initiated, with the details of shipper, details of carries, collection and delivery - postcode, latitude, longitude and time\\\n",
    "\n",
    "3. new_booking.csv: new shipments data booked by the shippers\n",
    "\n",
    "3. shipment_gps_data.csv: contains only shipments for which valid GPS information is captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8a151f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in input datasets\n",
    "\n",
    "gps_data = pd.read_csv(\"./Test/GPS_data.csv\")\n",
    "shipment = pd.read_csv(\"./Test/Shipment_bookings.csv\")\n",
    "new_booking = pd.read_csv(\"./Test/New_bookings.csv\")\n",
    "valid_ship_gps = pd.read_csv(\n",
    "    \"./shipment_gps_data.csv\"\n",
    ")  # shipments with valid GPS logs (re-using data created from Task 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e188c2",
   "metadata": {},
   "source": [
    "### Pre-processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialDelayPrediction:\n",
    "    \"\"\"\n",
    "    Class to build a model and predict if there is a potential delay or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, valid_data):\n",
    "        \"\"\"\n",
    "        To initialize all necessary class variables\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.valid_data = valid_data\n",
    "        self.mode = \"Train\"\n",
    "        self.columns_to_model = [\n",
    "            \"PROJECT_ID_ENCODED\",\n",
    "            \"CARRIER_DISPLAY_ID_ENCODED\",\n",
    "            \"VEHICLE_SIZE_ENCODED\",\n",
    "            \"VEHICLE_BUILD_UP_ENCODED\",\n",
    "            \"DAY_OF_FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "            \"DAY_OF_FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "            \"DAY_OF_LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "            \"DAY_OF_LAST_DELIVERY_SCHEDULE_LATEST\",\n",
    "            \"PERIOD_OF_FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "            \"PERIOD_OF_FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "            \"PERIOD_OF_LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "            \"PERIOD_OF_LAST_DELIVERY_SCHEDULE_LATEST\",\n",
    "            \"WEEKNUM_OF_FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "            \"WEEKNUM_OF_FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "            \"WEEKNUM_OF_LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "            \"WEEKNUM_OF_LAST_DELIVERY_SCHEDULE_LATEST\",\n",
    "            \"SCALED_DISTANCE_TO_DESTINATION\",\n",
    "            \"SCALED_TIME_TO_DESTINATION\",\n",
    "            \"DELAY?\",\n",
    "        ]\n",
    "\n",
    "    def format_time_stamp(self, input_col_name, output_col_name):\n",
    "        \"\"\"\n",
    "        Function to format the input time stamp to python understandable format\n",
    "        Args:\n",
    "            input_col_name (str): name of input column with time-stamp\n",
    "            output_col_name(str): name of new output column to fill the generated output\n",
    "        Returns:\n",
    "            Changes the dataframes with new columns\n",
    "        \"\"\"\n",
    "        self.data[output_col_name] = pd.to_datetime(\n",
    "            self.data[input_col_name].apply(\n",
    "                lambda x: (\n",
    "                    re.sub(\"T\", \" \", x[:-9])\n",
    "                    if \"Z\" not in x\n",
    "                    else re.sub(\"T\", \" \", x[:-5])\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_transit_details_google(self, x):\n",
    "        \"\"\"\n",
    "        Function to do API call to Google Distance API to retreive distance and ETA between two locations\n",
    "        Args:\n",
    "            x (row iter): row iter from the lambda function call\n",
    "        Returns:\n",
    "            distance in km\n",
    "            time in mins\n",
    "        \"\"\"\n",
    "        source = (\n",
    "            str(x[\"FIRST_COLLECTION_LATITUDE\"])\n",
    "            + \",\"\n",
    "            + str(x[\"FIRST_COLLECTION_LONGITUDE\"])\n",
    "        )\n",
    "        destination = (\n",
    "            str(x[\"LAST_DELIVERY_LATITUDE\"]) + \",\" + str(x[\"LAST_DELIVERY_LONGITUDE\"])\n",
    "        )\n",
    "        result = g_client.directions(\n",
    "            source, destination, mode=\"driving\", avoid=\"ferries\", transit_mode=\"bus\"\n",
    "        )\n",
    "        distance = result[0][\"legs\"][0][\"distance\"][\"value\"]\n",
    "        time = result[0][\"legs\"][0][\"duration\"][\"value\"]\n",
    "        return distance / 1000, time / 60\n",
    "\n",
    "    def get_label_encoding(self, column_name):\n",
    "        \"\"\"\n",
    "        Method to convert categorical values to numeric entities\n",
    "        Args:\n",
    "            column_name: column with the categorical values\n",
    "        Returns:\n",
    "            Dataframe column is replaced with the encoded values\n",
    "        \"\"\"\n",
    "        if self.mode == \"Train\":\n",
    "            encoder = preprocessing.OrdinalEncoder(\n",
    "                handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    "            )\n",
    "            self.data[str(column_name) + \"_ENCODED\"] = encoder.fit_transform(\n",
    "                np.array(self.data[column_name]).reshape(-1, 1)\n",
    "            )\n",
    "            pickle.dump(encoder, open(column_name + \"_encoder_\", \"wb\"))\n",
    "        else:\n",
    "            with open(column_name + \"_encoder_\", \"rb\") as pickle_file:\n",
    "                encoder = pickle.load(pickle_file)\n",
    "            self.data[str(column_name) + \"_ENCODED\"] = encoder.transform(\n",
    "                np.array(self.data[column_name]).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "    def get_day_of_week(self, column_name):\n",
    "        \"\"\"\n",
    "        Method to extract the day of the week (Sunday/Monday/Tuesday/Wednesday/Thrusday/Friday/Saturday)\n",
    "        Args:\n",
    "            column_name: column with the time stamp value\n",
    "        Returns:\n",
    "            New dataframe column is created and populated with extracted information\n",
    "        \"\"\"\n",
    "        self.data[\"DAY_OF_\" + re.sub(\"FORMATTED_\", \"\", column_name)] = self.data[\n",
    "            column_name\n",
    "        ].dt.dayofweek\n",
    "\n",
    "    def get_part_of_day(self, column_name):\n",
    "        \"\"\"\n",
    "        Method to extract the part of the week (Late night/Morning/Afternoon/Evening/Night)\n",
    "        Args:\n",
    "            column_name: column with the time stamp value\n",
    "        Returns:\n",
    "            New dataframe column is created and populated with extracted information\n",
    "        \"\"\"\n",
    "        self.data[\"PERIOD_OF_\" + re.sub(\"FORMATTED_\", \"\", column_name)] = (\n",
    "            self.data[column_name].dt.hour % 24 + 4\n",
    "        ) // 4\n",
    "\n",
    "    def get_weeknum_of_month(self, column_name):\n",
    "        \"\"\"\n",
    "        Method to extract the week number of the week (1/2/3/4)\n",
    "        Args:\n",
    "            column_name: column with the time stamp value\n",
    "        Returns:\n",
    "            New dataframe column is created and populated with extracted information\n",
    "        \"\"\"\n",
    "        self.data[\"WEEKNUM_OF_\" + re.sub(\"FORMATTED_\", \"\", column_name)] = (\n",
    "            self.data[column_name].dt.day - 1\n",
    "        ) // 7 + 1\n",
    "\n",
    "    def get_time_features(self):\n",
    "        \"\"\"\n",
    "        Method to call other methdos and extract the day of the week, part of the day and week number of the month\n",
    "        \"\"\"\n",
    "        time_columns = [\n",
    "            \"FORMATTED_FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "            \"FORMATTED_FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "            \"FORMATTED_LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "            \"FORMATTED_LAST_DELIVERY_SCHEDULE_LATEST\",\n",
    "        ]\n",
    "        for col in time_columns:\n",
    "            self.get_day_of_week(col)\n",
    "            self.get_part_of_day(col)\n",
    "            self.get_weeknum_of_month(col)\n",
    "\n",
    "    def get_scaled_values(self, column_name):\n",
    "        \"\"\"\n",
    "        Method to scale the values of columns based on min and max values\n",
    "        Args:\n",
    "            column_name: column with the numberic continous values\n",
    "        Return:\n",
    "            model which is used to build the scaling function\n",
    "        \"\"\"\n",
    "        if self.mode == \"Train\":\n",
    "            scaler = MinMaxScaler()\n",
    "            self.data[\"SCALED_\" + column_name] = scaler.fit_transform(\n",
    "                np.array(self.data[column_name]).reshape(-1, 1)\n",
    "            )\n",
    "            pickle.dump(scaler, open(column_name + \"_scaler_\", \"wb\"))\n",
    "        else:\n",
    "            with open(column_name + \"_scaler_\", \"rb\") as pickle_file:\n",
    "                scaler = pickle.load(pickle_file)\n",
    "            self.data[\"SCALED_\" + column_name] = scaler.fit_transform(\n",
    "                np.array(self.data[column_name]).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "    def get_google_estimate(self):\n",
    "        \"\"\"\n",
    "        Method used to do API call to Google directions API and retreive estimated distance and time\n",
    "\n",
    "        \"\"\"\n",
    "        self.data[[\"DISTANCE_TO_DESTINATION\", \"TIME_TO_DESTINATION\"]] = self.data.apply(\n",
    "            lambda x: self.get_transit_details_google(x),\n",
    "            axis=\"columns\",\n",
    "            result_type=\"expand\",\n",
    "        )\n",
    "\n",
    "    def get_target_variable(self):\n",
    "        \"\"\"\n",
    "        Method to join the shipment data with the extracted data with delivery indictor from previous question\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = self.valid_data.merge(self.data, how=\"inner\", on=\"SHIPMENT_NUMBER\")\n",
    "        self.data[\"DELAY?\"] = self.data[\"INDICATOR\"].apply(\n",
    "            lambda x: (\n",
    "                0 if (x == \"advanced_delivery\") or (x == \"threshold_delivery\") else 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_test_split(self):\n",
    "        \"\"\"\n",
    "        Method to split the data into train and test to perfrom model building process on train\n",
    "        and validation later on test\n",
    "        \"\"\"\n",
    "        # print(self.data.columns)\n",
    "        self.train = self.data[self.columns_to_model]\n",
    "        X = self.train.iloc[:, :-1]\n",
    "        y = self.train[\"DELAY?\"]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.33, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Method to build the classification model using Logistic Regression algorithm\n",
    "        \"\"\"\n",
    "        # instantiate the model (using the default parameters)\n",
    "        logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "        # fit the model with data\n",
    "        logreg.fit(self.X_train, self.y_train)\n",
    "        pickle.dump(logreg, open(\"logreg\", \"wb\"))\n",
    "\n",
    "    def validation(self):\n",
    "        \"\"\"\n",
    "        Method to load the built model and validate on the test split\n",
    "        \"\"\"\n",
    "        with open(\"logreg\", \"rb\") as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "        self.y_pred = model.predict(self.X_test)\n",
    "\n",
    "    def get_prediction(self):\n",
    "        \"\"\"\n",
    "        Method to load the built model and generate predictions on unseen data\n",
    "        \"\"\"\n",
    "        with open(\"logreg\", \"rb\") as pickle_file:\n",
    "            model = pickle.load(pickle_file)\n",
    "        self.predictions = model.predict(self.data[self.columns_to_model[:-1]])\n",
    "\n",
    "    def model_performance_metrics(self):\n",
    "        \"\"\"\n",
    "        Method to generate the output metrics(accuracy, precision, recall) on the predictions made by the model\n",
    "        \"\"\"\n",
    "        target_names = [\"0\", \"1\"]\n",
    "        print(\n",
    "            classification_report(self.y_test, self.y_pred, target_names=target_names)\n",
    "        )\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Method to format the input data on required columns\n",
    "        \"\"\"\n",
    "        self.format_time_stamp(\n",
    "            \"FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "            \"FORMATTED_FIRST_COLLECTION_SCHEDULE_EARLIEST\",\n",
    "        )\n",
    "        self.format_time_stamp(\n",
    "            \"FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "            \"FORMATTED_FIRST_COLLECTION_SCHEDULE_LATEST\",\n",
    "        )\n",
    "        self.format_time_stamp(\n",
    "            \"LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "            \"FORMATTED_LAST_DELIVERY_SCHEDULE_EARLIEST\",\n",
    "        )\n",
    "        self.format_time_stamp(\n",
    "            \"LAST_DELIVERY_SCHEDULE_LATEST\", \"FORMATTED_LAST_DELIVERY_SCHEDULE_LATEST\"\n",
    "        )\n",
    "\n",
    "    def feature_eng(self):\n",
    "        \"\"\"\n",
    "        Method to call required methods to perform feature engineering\n",
    "        \"\"\"\n",
    "        self.get_label_encoding(\"PROJECT_ID\")\n",
    "        self.get_label_encoding(\"CARRIER_DISPLAY_ID\")\n",
    "        self.get_label_encoding(\"VEHICLE_SIZE\")\n",
    "        self.get_label_encoding(\"VEHICLE_BUILD_UP\")\n",
    "        self.get_time_features()\n",
    "        self.get_google_estimate()\n",
    "        self.get_scaled_values(\"DISTANCE_TO_DESTINATION\")\n",
    "        self.get_scaled_values(\"TIME_TO_DESTINATION\")\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Method to control the entire model training process and followed by validation on test data\n",
    "        \"\"\"\n",
    "        self.preprocess()\n",
    "        self.feature_eng()\n",
    "        self.get_target_variable()\n",
    "        self.train_test_split()\n",
    "        self.train_model()\n",
    "        self.validation()\n",
    "        self.model_performance_metrics\n",
    "\n",
    "    def test(self, data):\n",
    "        \"\"\"\n",
    "        Method to control the entire model prediction process on the unseen data\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.mode = \"Test\"\n",
    "        self.data.rename(\n",
    "            columns={\"SHIPPER_ID\": \"PROJECT_ID\", \"CARRIER_ID\": \"CARRIER_DISPLAY_ID\"},\n",
    "            inplace=True,\n",
    "        )\n",
    "        self.preprocess()\n",
    "        self.feature_eng()\n",
    "        self.get_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_model = PotentialDelayPrediction(shipment, valid_ship_gps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9147ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_model.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c947bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_model.test(new_booking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
